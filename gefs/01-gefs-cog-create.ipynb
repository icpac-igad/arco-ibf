{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78e092-e9be-4731-96a8-9c5e74cdd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rio_cogeo.cogeo import cog_translate\n",
    "from rio_cogeo.profiles import cog_profiles\n",
    "from rasterio.io import MemoryFile\n",
    "import xarray as xr\n",
    "import requests\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "import fsspec\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import argparse\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4ffe6-1e56-41a5-af7e-c9c5ce73313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Default configuration values\n",
    "DEFAULT_CONFIG = {\n",
    "    'SERVICE_ACCOUNT_KEY': 'coiled-data-e4drr.json',\n",
    "    'BUCKET_NAME': 'gefs-wgl',\n",
    "    'EXTENT': [21.85, 51.50, 23.14, -11.72],\n",
    "    'START_TIME': '0h',\n",
    "    'END_TIME': '21h',\n",
    "    'STORE': 'https://data.dynamical.org/noaa/gefs/forecast-35-day/latest.zarr'\n",
    "}\n",
    "\n",
    "def load_environment(env_file=None):\n",
    "    \"\"\"\n",
    "    Load environment variables from specified .env file\n",
    "    If no file is specified, it will try to find .env in the current directory\n",
    "    \"\"\"\n",
    "    # If env_file is specified, load it\n",
    "    if env_file and os.path.exists(env_file):\n",
    "        print(f\"Loading environment from: {env_file}\")\n",
    "        load_dotenv(env_file)\n",
    "    # Otherwise, try to find .env in the current directory\n",
    "    else:\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            print(f\"Loading environment from: {env_path}\")\n",
    "            load_dotenv(env_path)\n",
    "        else:\n",
    "            print(\"Warning: No .env file found, using default values\")\n",
    "\n",
    "    # Load configuration using DEFAULT_CONFIG as fallback values\n",
    "    config = {\n",
    "        'SERVICE_ACCOUNT_KEY': os.getenv('SERVICE_ACCOUNT_KEY', DEFAULT_CONFIG['SERVICE_ACCOUNT_KEY']),\n",
    "        'BUCKET_NAME': os.getenv('BUCKET_NAME', DEFAULT_CONFIG['BUCKET_NAME']),\n",
    "        'EXTENT': [\n",
    "            float(os.getenv('EXTENT_X1', str(DEFAULT_CONFIG['EXTENT'][0]))),\n",
    "            float(os.getenv('EXTENT_X2', str(DEFAULT_CONFIG['EXTENT'][1]))),\n",
    "            float(os.getenv('EXTENT_Y1', str(DEFAULT_CONFIG['EXTENT'][2]))),\n",
    "            float(os.getenv('EXTENT_Y2', str(DEFAULT_CONFIG['EXTENT'][3])))\n",
    "        ],\n",
    "        'START_TIME': os.getenv('START_TIME', DEFAULT_CONFIG['START_TIME']),\n",
    "        'END_TIME': os.getenv('END_TIME', DEFAULT_CONFIG['END_TIME']),\n",
    "        'STORE': os.getenv('STORE', DEFAULT_CONFIG['STORE'])\n",
    "    }\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6404c7-9375-4f64-9343-32984f435342",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=load_environment(env_file=None)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db76930-14fa-4c8a-a755-24d16789afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create directory if it doesn't exist\n",
    "\"\"\"\n",
    "def ensure_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return directory\n",
    "\n",
    "\"\"\"\n",
    "Delete temporary file\n",
    "\"\"\"\n",
    "def delete_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    return\n",
    "\n",
    "\"\"\"\n",
    "Upload temporary file to Google Storage Bucket\n",
    "\"\"\"\n",
    "def upload_tif_to_gcs(bucket_name, source_file_path, destination_blob_name, service_account_path):\n",
    "    with open(service_account_path, 'r') as f:\n",
    "        service_account_info = json.load(f)\n",
    "    client = storage.Client.from_service_account_info(service_account_info)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"Uploaded to gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Merge u & v tif files to single raster file (Band 1 & 2)\n",
    "\"\"\"\n",
    "def merge_uv(file1, file2, output_path):\n",
    "    with rasterio.open(file1) as src1, rasterio.open(file2) as src2:\n",
    "        band1 = src1.read(1)\n",
    "        band2 = src2.read(1)\n",
    "\n",
    "        profile = src1.profile\n",
    "        profile.update(count=2)\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(band1, 1)\n",
    "            dst.write(band2, 2)\n",
    "    return output_path\n",
    "\n",
    "def reproject_tif(src_path, dst_path):\n",
    "    dst_crs = 'EPSG:4326'\n",
    "    with rasterio.open(src_path) as src:\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds\n",
    "        )\n",
    "        profile = src.profile.copy()\n",
    "        profile.update({\n",
    "            \"crs\": dst_crs,\n",
    "            \"transform\": transform,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"tiled\": True,\n",
    "            \"blockxsize\": 512,\n",
    "            \"blockysize\": 512,\n",
    "            \"compress\": \"deflate\",\n",
    "            \"interleave\": \"band\"\n",
    "        })\n",
    "\n",
    "        with MemoryFile() as memfile:\n",
    "            with memfile.open(**profile) as mem:\n",
    "                for i in range(1, src.count + 1):\n",
    "                    reproject(\n",
    "                        source=rasterio.band(src, i),\n",
    "                        destination=rasterio.band(mem, i),\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=dst_crs,\n",
    "                        resampling=Resampling.nearest\n",
    "                    )\n",
    "\n",
    "            # COG profile with tiling\n",
    "            cog_profile = cog_profiles.get(\"deflate\")\n",
    "\n",
    "            cog_translate(\n",
    "                memfile, dst_path,\n",
    "                cog_profile,\n",
    "                in_memory=True\n",
    "            )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Save dataset as tif\n",
    "\"\"\"\n",
    "def process_param_to_tif(e, t, param_data, file_path):\n",
    "    td = np.timedelta64(t, 'ns')\n",
    "    hours = td / np.timedelta64(1, 'h')\n",
    "    param_data_sel = param_data.sel(lead_time=f\"{int(hours)}hr\").sel(ensemble_member=e)\n",
    "    param_array = xr.DataArray(\n",
    "        param_data_sel.values[:, :].astype(np.float32),\n",
    "        dims=['latitude', 'longitude'],\n",
    "        coords={\n",
    "            'latitude': param_data_sel.latitude.values,\n",
    "            'longitude': param_data_sel.longitude.values\n",
    "        }\n",
    "    )\n",
    "    param_array.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "    tmp_file_path = file_path + \"_tmp.tif\"\n",
    "    param_array.rio.to_raster(\n",
    "        tmp_file_path,\n",
    "        driver='COG',\n",
    "        compress='LZW',\n",
    "        dtype='float32',\n",
    "        nodata=np.nan\n",
    "    )\n",
    "    reproject_tif(tmp_file_path, file_path)\n",
    "    delete_file(tmp_file_path)\n",
    "    return file_path\n",
    "\n",
    "\"\"\"\n",
    "Filter dataset for given parameter\n",
    "\"\"\"\n",
    "def filter_param(ds, param,config):\n",
    "       \n",
    "    param_subset = ds[param].sel(\n",
    "        # No need for init_time selection since the dataset is already filtered\n",
    "        lead_time=slice(config['START_TIME'], config['END_TIME']),\n",
    "        latitude=slice(config['EXTENT'][2], config['EXTENT'][3]),\n",
    "        longitude=slice(config['EXTENT'][0], config['EXTENT'][1]),\n",
    "    )\n",
    "    param_data = param_subset.load()\n",
    "    return param_data\n",
    "\"\"\"\n",
    "Process single parameter (Scalar)\n",
    "\"\"\"\n",
    "def process_scalar(ds,latest_init_time, param_obj, base_folder,config):\n",
    "    param_data = filter_param(ds,param_obj['params'][0], config)\n",
    "    print(param_data)\n",
    "    for e in param_data.ensemble_member.values:\n",
    "        for t in param_data.lead_time.values:\n",
    "            latest_init_datetime = pd.to_datetime(latest_init_time)\n",
    "            # Now add the timedelta\n",
    "            forecast_datetime = latest_init_datetime + t\n",
    "            # Format file name as param_YYYYMMDDHH.tif\n",
    "            file_name = f\"{param_obj['params'][0]}_{forecast_datetime.strftime('%Y%m%d%H')}.tif\"\n",
    "            if e > 0:  # Add ensemble member number if not the first ensemble\n",
    "                file_name = f\"{param_obj['params'][0]}_{forecast_datetime.strftime('%Y%m%d%H')}_{e}.tif\"\n",
    "                \n",
    "            file_path = os.path.join(base_folder, file_name)\n",
    "            \n",
    "            process_param_to_tif(e, t, param_data, file_path)\n",
    "            print(file_path)\n",
    "            \n",
    "            destination_blob_path = os.path.join(init_date_folder, file_name)\n",
    "            upload_tif_to_gcs(\n",
    "                bucket_name=config['BUCKET_NAME'],\n",
    "                source_file_path=file_path,\n",
    "                destination_blob_name=destination_blob_path,\n",
    "                service_account_path=config['SERVICE_ACCOUNT_KEY']\n",
    "            )\n",
    "            #delete_file(file_path)\n",
    "\n",
    "\"\"\"\n",
    "Process vector parameter\n",
    "\"\"\"\n",
    "def process_vector(ds,latest_init_time, param_obj, base_folder, config):\n",
    "    param_data_u = filter_param(ds, param_obj['params'][0], config)\n",
    "    param_data_v = filter_param(ds, param_obj['params'][1], config)\n",
    "\n",
    "    for e in param_data_u.ensemble_member.values:\n",
    "        for t in param_data_u.lead_time.values:\n",
    "            latest_init_datetime = pd.to_datetime(latest_init_time)\n",
    "            # Now add the timedelta\n",
    "            forecast_datetime = latest_init_datetime + t\n",
    "            #forecast_datetime = pd.to_datetime(str(latest_init_time + t))\n",
    "            # Format temporary file names\n",
    "            temp_u_name = f\"{param_obj['params'][0]}_{forecast_datetime.strftime('%Y%m%d%H')}_tmp.tif\"\n",
    "            temp_v_name = f\"{param_obj['params'][1]}_{forecast_datetime.strftime('%Y%m%d%H')}_tmp.tif\"\n",
    "            \n",
    "            file_path_u = os.path.join(base_folder, temp_u_name)\n",
    "            file_path_v = os.path.join(base_folder, temp_v_name)\n",
    "\n",
    "            process_param_to_tif(e, t, param_data_u, file_path_u)\n",
    "            process_param_to_tif(e, t, param_data_v, file_path_v)\n",
    "\n",
    "            # Format output file name as param_YYYYMMDDHH.tif\n",
    "            file_name = f\"{param_obj['id']}_{forecast_datetime.strftime('%Y%m%d%H')}.tif\"\n",
    "            if e > 0:  # Add ensemble member number if not the first ensemble\n",
    "                file_name = f\"{param_obj['id']}_{forecast_datetime.strftime('%Y%m%d%H')}_{e}.tif\"\n",
    "                \n",
    "            file_path = os.path.join(base_folder, file_name)\n",
    "            \n",
    "            merge_uv(file_path_u, file_path_v, file_path)\n",
    "            print(file_path)\n",
    "            \n",
    "            destination_blob_path = os.path.join(init_date_folder, file_name)\n",
    "            upload_tif_to_gcs(\n",
    "                bucket_name=config['BUCKET_NAME'],\n",
    "                source_file_path=file_path,\n",
    "                destination_blob_name=destination_blob_path,\n",
    "                service_account_path=config['SERVICE_ACCOUNT_KEY']\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary files\n",
    "            #delete_file(file_path)\n",
    "            #delete_file(file_path_u)\n",
    "            #delete_file(file_path_v)\n",
    "\n",
    "def get_forecast_data(ds_global, date=None):\n",
    "    \"\"\"\n",
    "    Get forecast data for a specific date or the latest available\n",
    "    \n",
    "    Parameters:\n",
    "    - ds_global: xarray dataset containing forecast data\n",
    "    - date: string in YYYYMMDD format, or None for latest available\n",
    "    \n",
    "    Returns:\n",
    "    - dataset and initialization time\n",
    "    \"\"\"\n",
    "    # If no specific date is requested, use the latest\n",
    "    if date is None or date == \"\":\n",
    "        latest_init_time = ds_global.init_time.values[-1]\n",
    "        # Subset to the latest init time\n",
    "        ds_subset = ds_global.sel(init_time=latest_init_time)\n",
    "        return ds_subset, latest_init_time\n",
    "    \n",
    "    # Convert input date to datetime for comparison\n",
    "    try:\n",
    "        target_date = pd.to_datetime(date, format=\"%Y%m%d\")\n",
    "        \n",
    "        # Find the closest initialization time to the requested date\n",
    "        init_times = pd.to_datetime(ds_global.init_time.values)\n",
    "        time_diffs = abs(init_times - target_date)\n",
    "        closest_idx = time_diffs.argmin()\n",
    "        selected_init_time = ds_global.init_time.values[closest_idx]\n",
    "        \n",
    "        # Subset to the selected init time\n",
    "        ds_subset = ds_global.sel(init_time=selected_init_time)\n",
    "        \n",
    "        # Print info about selected date\n",
    "        print(f\"Requested date: {target_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Using initialization time: {pd.to_datetime(selected_init_time).strftime('%Y-%m-%d %H:%M')}\")\n",
    "        \n",
    "        return ds_subset, selected_init_time\n",
    "        \n",
    "    except ValueError:\n",
    "        print(f\"Warning: Invalid date format '{date}'. Using latest available date.\")\n",
    "        latest_init_time = ds_global.init_time.values[-1]\n",
    "        # Subset to the latest init time\n",
    "        ds_subset = ds_global.sel(init_time=latest_init_time)\n",
    "        print(f\"Using latest initialization time: {pd.to_datetime(latest_init_time).strftime('%Y-%m-%d %H:%M')}\")\n",
    "        return ds_subset, latest_init_time\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fe2a1-e6d2-46e1-97d2-b7842a7c8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_global = xr.open_zarr(config['STORE'], decode_timedelta=True, chunks=None, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081cedf-4089-4914-b501-162794d264ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_global\n",
    "latest_init_time = ds.init_time.values[-1]\n",
    "latest_init_time='20250407'\n",
    "# Format the initialization date as YYYYMMDDz00\n",
    "init_datetime = pd.to_datetime(latest_init_time)\n",
    "init_date_str = init_datetime.strftime('%Y%m%d') + 'z00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cf6d6-cc87-4560-ae67-5b78d49e6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, init_time=get_forecast_data(ds_global, date=latest_init_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd54e60-c05a-4ec2-b870-e46fb8d02836",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='./'\n",
    "init_date_folder = init_date_str\n",
    "\n",
    "# Create a local temporary directory for processing\n",
    "temp_dir = os.path.join(output_dir, init_date_str)\n",
    "ensure_dir(temp_dir)\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'id': 'precipitation_surface',\n",
    "        'type': 'scalar',\n",
    "        'dType': 'float',\n",
    "        'params': ['precipitation_surface']\n",
    "    },\n",
    "    {\n",
    "        'id': 'wind_10m',\n",
    "        'type': 'vector',\n",
    "        'dType': 'float',\n",
    "        'params': ['wind_u_10m', 'wind_v_10m']\n",
    "    },\n",
    "    {\n",
    "        'id': 'temperature_2m',\n",
    "        'type': 'scalar',\n",
    "        'dType': 'float',\n",
    "        'params': ['temperature_2m']\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Processing data for initialization date: {init_date_str}\")\n",
    "print(f\"Output folder: {init_date_folder}\")\n",
    "\n",
    "try:\n",
    "    for param_obj in params:\n",
    "        print(f\"Processing {param_obj['id']}...\")\n",
    "        if param_obj['type'] == 'vector':\n",
    "            process_vector(ds, latest_init_time, param_obj, temp_dir,config)\n",
    "        else:\n",
    "            process_scalar(ds, latest_init_time, param_obj, temp_dir,config)\n",
    "            \n",
    "    print(\"Processing complete!\")\n",
    "finally:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
